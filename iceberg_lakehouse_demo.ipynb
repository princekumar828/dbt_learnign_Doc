{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4b480f2",
   "metadata": {},
   "source": [
    "# Iceberg Lakehouse with Spark, MinIO, and Nessie\n",
    "\n",
    "This notebook demonstrates how to work with Apache Iceberg tables using Spark as the query engine, MinIO for storage, and Nessie as the catalog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4affd784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Spark session...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/14 15:59:45 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n",
      "java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "java.base/java.lang.Thread.run(Thread.java:1575)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:588)\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2446)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2446)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:339)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 80\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating new Spark session...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Create Spark session with proper configuration for Java 17 and Iceberg\u001b[39;00m\n\u001b[1;32m     20\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIcebergLakehouseDemo\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.driver.host\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.driver.bindAddress\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.driver.extraJavaOptions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     26\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/java.lang=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/java.lang.invoke=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/java.lang.reflect=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/java.io=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/java.net=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/java.nio=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/java.util=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/java.util.concurrent=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/sun.nio.ch=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/sun.nio.cs=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/sun.security.action=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/sun.util.calendar=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-Djavax.security.auth.useSubjectCredsOnly=false\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.executor.extraJavaOptions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     42\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/java.lang=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/java.lang.invoke=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/java.lang.reflect=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/java.io=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/java.net=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/java.nio=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/java.util=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/java.util.concurrent=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/sun.nio.ch=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/sun.nio.cs=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/sun.security.action=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.base/sun.util.calendar=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-Djavax.security.auth.useSubjectCredsOnly=false\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.extensions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.nessie\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.iceberg.spark.SparkCatalog\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.nessie.type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrest\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.nessie.uri\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://nessie:19120/api/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.nessie.warehouse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3a://warehouse/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.nessie.io-impl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.iceberg.aws.s3.S3FileIO\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.nessie.s3.endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://minio:9000\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.nessie.s3.access-key-id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madmin\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.nessie.s3.secret-access-key\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword123\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.nessie.s3.path-style-access\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.hadoop.fs.s3a.endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://minio:9000\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.hadoop.fs.s3a.access.key\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madmin\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.hadoop.fs.s3a.secret.key\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword123\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.hadoop.fs.s3a.path.style.access\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.hadoop.fs.s3a.impl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.hadoop.fs.s3a.S3AFileSystem\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.hadoop.fs.s3a.connection.ssl.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.defaultCatalog\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnessie\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.jars\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     75\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.4.3.jar,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/spark/jars/iceberg-aws-bundle-1.4.3.jar,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/spark/jars/nessie-iceberg-1.0.1.jar,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/spark/jars/hadoop-aws-3.3.4.jar,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/spark/jars/aws-java-sdk-bundle-1.12.367.jar\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Spark session created successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📊 Spark version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspark\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/pyspark/sql/session.py:556\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/pyspark/core/context.py:523\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 523\u001b[0m         SparkContext(conf\u001b[38;5;241m=\u001b[39mconf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/pyspark/core/context.py:207\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    205\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    208\u001b[0m         master,\n\u001b[1;32m    209\u001b[0m         appName,\n\u001b[1;32m    210\u001b[0m         sparkHome,\n\u001b[1;32m    211\u001b[0m         pyFiles,\n\u001b[1;32m    212\u001b[0m         environment,\n\u001b[1;32m    213\u001b[0m         batchSize,\n\u001b[1;32m    214\u001b[0m         serializer,\n\u001b[1;32m    215\u001b[0m         conf,\n\u001b[1;32m    216\u001b[0m         jsc,\n\u001b[1;32m    217\u001b[0m         profiler_cls,\n\u001b[1;32m    218\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    219\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    220\u001b[0m     )\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/pyspark/core/context.py:300\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_context(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf\u001b[38;5;241m.\u001b[39m_jconf)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/pyspark/core/context.py:429\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mJavaSparkContext(jconf)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/py4j/java_gateway.py:1627\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1621\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1622\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1623\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1624\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1626\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1627\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1628\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn)\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/py4j/protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:588)\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2446)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2446)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:339)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Set required environment variables for Java 17 compatibility\n",
    "os.environ['HADOOP_USER_NAME'] = 'jovyan'\n",
    "os.environ['JAVA_TOOL_OPTIONS'] = '--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'\n",
    "\n",
    "# Stop any existing Spark session\n",
    "try:\n",
    "    if 'spark' in locals() and hasattr(spark, 'sparkContext'):\n",
    "        print(\"Stopping existing Spark session...\")\n",
    "        spark.stop()\n",
    "        print(\"Spark session stopped.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Creating new Spark session...\")\n",
    "\n",
    "# Create Spark session with proper configuration for Java 17 and Iceberg\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergLakehouseDemo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"localhost\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \n",
    "            \"--add-opens=java.base/java.lang=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/java.lang.invoke=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/java.lang.reflect=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/java.io=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/java.net=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/java.nio=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/java.util=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/java.util.concurrent=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/sun.nio.cs=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/sun.security.action=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/sun.util.calendar=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED \"\n",
    "            \"-Djavax.security.auth.useSubjectCredsOnly=false\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \n",
    "            \"--add-opens=java.base/java.lang=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/java.lang.invoke=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/java.lang.reflect=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/java.io=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/java.net=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/java.nio=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/java.util=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/java.util.concurrent=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/sun.nio.cs=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/sun.security.action=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.base/sun.util.calendar=ALL-UNNAMED \"\n",
    "            \"--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED \"\n",
    "            \"-Djavax.security.auth.useSubjectCredsOnly=false\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.nessie.type\", \"rest\") \\\n",
    "    .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie:19120/api/v1\") \\\n",
    "    .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://warehouse/\") \\\n",
    "    .config(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .config(\"spark.sql.catalog.nessie.s3.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.sql.catalog.nessie.s3.access-key-id\", \"admin\") \\\n",
    "    .config(\"spark.sql.catalog.nessie.s3.secret-access-key\", \"password123\") \\\n",
    "    .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"password123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.defaultCatalog\", \"nessie\") \\\n",
    "    .config(\"spark.jars\", \n",
    "            \"/opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.4.3.jar,\"\n",
    "            \"/opt/spark/jars/iceberg-aws-bundle-1.4.3.jar,\"\n",
    "            \"/opt/spark/jars/nessie-iceberg-1.0.1.jar,\"\n",
    "            \"/opt/spark/jars/hadoop-aws-3.3.4.jar,\"\n",
    "            \"/opt/spark/jars/aws-java-sdk-bundle-1.12.367.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark session created successfully!\")\n",
    "print(f\"📊 Spark version: {spark.version}\")\n",
    "print(f\"🎯 Spark master: {spark.sparkContext.master}\")\n",
    "\n",
    "# Test basic connectivity\n",
    "try:\n",
    "    catalogs = spark.sql(\"SHOW CATALOGS\").collect()\n",
    "    print(f\"📋 Available catalogs: {[row.catalog for row in catalogs]}\")\n",
    "    if 'nessie' in [row.catalog for row in catalogs]:\n",
    "        print(\"✅ Nessie catalog is available!\")\n",
    "    else:\n",
    "        print(\"⚠️  Nessie catalog not found in catalog list\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error listing catalogs: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "063c81b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Testing service connectivity...\n",
      "✅ MinIO is accessible\n",
      "✅ Nessie is accessible\n",
      "❌ Spark Master is not accessible\n",
      "\n",
      "🚀 Proceeding with Spark session creation...\n"
     ]
    }
   ],
   "source": [
    "# Test service connectivity first\n",
    "import requests\n",
    "from time import sleep\n",
    "\n",
    "print(\"🔍 Testing service connectivity...\")\n",
    "\n",
    "# Test MinIO\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:9000/minio/health/live\", timeout=5)\n",
    "    print(\"✅ MinIO is accessible\")\n",
    "except:\n",
    "    print(\"❌ MinIO is not accessible\")\n",
    "\n",
    "# Test Nessie\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:19120/api/v1/config\", timeout=5)\n",
    "    print(\"✅ Nessie is accessible\")\n",
    "except:\n",
    "    print(\"❌ Nessie is not accessible\")\n",
    "\n",
    "# Test Spark Master\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:8080\", timeout=5)\n",
    "    print(\"✅ Spark Master is accessible\")\n",
    "except:\n",
    "    print(\"❌ Spark Master is not accessible\")\n",
    "\n",
    "print(\"\\n🚀 Proceeding with Spark session creation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6acbb61f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test connection to Nessie catalog\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSHOW CATALOGS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m      3\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSE CATALOG nessie\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSHOW NAMESPACES\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Test connection to Nessie catalog\n",
    "print(\"📊 Testing Nessie catalog connection...\")\n",
    "\n",
    "try:\n",
    "    # Show available catalogs\n",
    "    catalogs_df = spark.sql(\"SHOW CATALOGS\")\n",
    "    print(\"Available catalogs:\")\n",
    "    catalogs_df.show()\n",
    "    \n",
    "    # Switch to Nessie catalog\n",
    "    spark.sql(\"USE CATALOG nessie\")\n",
    "    print(\"✅ Successfully switched to Nessie catalog\")\n",
    "    \n",
    "    # Show namespaces\n",
    "    namespaces_df = spark.sql(\"SHOW NAMESPACES\")\n",
    "    print(\"Available namespaces in Nessie:\")\n",
    "    namespaces_df.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error connecting to Nessie catalog: {e}\")\n",
    "    print(\"Continuing with spark_catalog...\")\n",
    "    spark.sql(\"USE CATALOG spark_catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4600c100",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create a namespace (database) in Nessie\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE NAMESPACE IF NOT EXISTS lakehouse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSE lakehouse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated and switched to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlakehouse\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m namespace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a namespace (database) in Nessie\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS lakehouse\")\n",
    "spark.sql(\"USE lakehouse\")\n",
    "print(\"Created and switched to 'lakehouse' namespace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType, DateType\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "from datetime import date\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"price\", DecimalType(10, 2), True),\n",
    "    StructField(\"launch_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Create sample data\n",
    "data = [\n",
    "    (1, \"Laptop Pro\", \"Electronics\", 1299.99, date(2024, 1, 15)),\n",
    "    (2, \"Wireless Mouse\", \"Electronics\", 29.99, date(2024, 2, 1)),\n",
    "    (3, \"Coffee Mug\", \"Kitchen\", 12.50, date(2024, 1, 20)),\n",
    "    (4, \"Desk Chair\", \"Furniture\", 249.99, date(2024, 3, 10)),\n",
    "    (5, \"Notebook\", \"Stationery\", 5.99, date(2024, 2, 15))\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "print(f\"Created DataFrame with {df.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abd890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Iceberg table\n",
    "df.writeTo(\"nessie.lakehouse.products\").using(\"iceberg\").create()\n",
    "print(\"Created Iceberg table: nessie.lakehouse.products\")\n",
    "\n",
    "# Verify table creation\n",
    "spark.sql(\"SHOW TABLES IN lakehouse\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b17571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the Iceberg table\n",
    "result = spark.sql(\"SELECT * FROM nessie.lakehouse.products ORDER BY price DESC\")\n",
    "result.show()\n",
    "\n",
    "# Show table metadata\n",
    "spark.sql(\"DESCRIBE EXTENDED nessie.lakehouse.products\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99a3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert more data (demonstrating append operation)\n",
    "new_data = [\n",
    "    (6, \"Smartphone\", \"Electronics\", 699.99, date(2024, 4, 1)),\n",
    "    (7, \"Table Lamp\", \"Furniture\", 45.00, date(2024, 3, 25))\n",
    "]\n",
    "\n",
    "new_df = spark.createDataFrame(new_data, schema)\n",
    "new_df.writeTo(\"nessie.lakehouse.products\").using(\"iceberg\").append()\n",
    "\n",
    "print(\"Appended new data to the table\")\n",
    "spark.sql(\"SELECT COUNT(*) as total_products FROM nessie.lakehouse.products\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe93fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update existing data (demonstrating merge operation)\n",
    "spark.sql(\"\"\"\n",
    "    MERGE INTO nessie.lakehouse.products AS target\n",
    "    USING (\n",
    "        SELECT 1 as product_id, 1199.99 as new_price\n",
    "    ) AS source\n",
    "    ON target.product_id = source.product_id\n",
    "    WHEN MATCHED THEN UPDATE SET price = source.new_price\n",
    "\"\"\")\n",
    "\n",
    "print(\"Updated product price\")\n",
    "spark.sql(\"SELECT * FROM nessie.lakehouse.products WHERE product_id = 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8b868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show table history (time travel capability)\n",
    "spark.sql(\"SELECT * FROM nessie.lakehouse.products.history\").show(truncate=False)\n",
    "\n",
    "# Show snapshots\n",
    "spark.sql(\"SELECT * FROM nessie.lakehouse.products.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1dd2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate time travel - query previous version\n",
    "# Get the first snapshot ID\n",
    "snapshots = spark.sql(\"SELECT snapshot_id FROM nessie.lakehouse.products.snapshots ORDER BY committed_at LIMIT 1\")\n",
    "first_snapshot = snapshots.collect()[0]['snapshot_id']\n",
    "\n",
    "print(f\"Querying snapshot: {first_snapshot}\")\n",
    "spark.sql(f\"SELECT COUNT(*) as count_at_first_snapshot FROM nessie.lakehouse.products VERSION AS OF {first_snapshot}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e5e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytics queries\n",
    "print(\"=== Product Analytics ===\")\n",
    "\n",
    "# Products by category\n",
    "spark.sql(\"\"\"\n",
    "    SELECT category, \n",
    "           COUNT(*) as product_count,\n",
    "           AVG(price) as avg_price,\n",
    "           MIN(price) as min_price,\n",
    "           MAX(price) as max_price\n",
    "    FROM nessie.lakehouse.products \n",
    "    GROUP BY category\n",
    "    ORDER BY product_count DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# Recent launches\n",
    "spark.sql(\"\"\"\n",
    "    SELECT product_name, category, price, launch_date\n",
    "    FROM nessie.lakehouse.products\n",
    "    WHERE launch_date >= '2024-03-01'\n",
    "    ORDER BY launch_date DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba181ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up (optional)\n",
    "# spark.sql(\"DROP TABLE IF EXISTS nessie.lakehouse.products\")\n",
    "# spark.sql(\"DROP NAMESPACE IF EXISTS lakehouse\")\n",
    "\n",
    "# Stop Spark session\n",
    "# spark.stop()\n",
    "print(\"Notebook completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
